{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252094ad",
   "metadata": {},
   "source": [
    "# DQN Agent for Hangman\n",
    "\n",
    "**Deep Q-Network (DQN) Implementation**\n",
    "\n",
    "This notebook implements a Deep Q-Learning agent for the Hangman game using PyTorch.\n",
    "\n",
    "## Key Features:\n",
    "- **State Representation**: One-hot encoded masked word + guessed letters bitmap + lives remaining\n",
    "- **Action Space**: 26 letters (a-z)\n",
    "- **Architecture**: Multi-layer perceptron with experience replay\n",
    "- **Training**: Epsilon-greedy exploration with target network updates\n",
    "\n",
    "**Course:** UE23CS352A - Machine Learning Lab  \n",
    "**Date:** November 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a801f1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bbc1988",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, Counter\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c6086",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus and test data\n",
    "with open('Data/Data/corpus.txt', 'r') as f:\n",
    "    corpus = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "with open('Data/Data/test.txt', 'r') as f:\n",
    "    test_words = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Corpus: {len(corpus)} words ({len(set(corpus))} unique)\")\n",
    "print(f\"Test: {len(test_words)} words ({len(set(test_words))} unique)\")\n",
    "print(f\"Overlap: {len(set(corpus) & set(test_words))} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac99a62b",
   "metadata": {},
   "source": [
    "## 3. Hangman Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c2e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanEnv:\n",
    "    \"\"\"Hangman game environment for DQN\"\"\"\n",
    "    \n",
    "    def __init__(self, word, max_lives=6):\n",
    "        self.word = word.lower()\n",
    "        self.max_lives = max_lives\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the game state\"\"\"\n",
    "        self.guessed = set()\n",
    "        self.lives = self.max_lives\n",
    "        self.masked = '_' * len(self.word)\n",
    "        self.done = False\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"Get current state representation\n",
    "        Returns: dict with 'masked', 'guessed', 'lives', 'available'\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'masked': self.masked,\n",
    "            'guessed': self.guessed.copy(),\n",
    "            'lives': self.lives,\n",
    "            'available': set(string.ascii_lowercase) - self.guessed\n",
    "        }\n",
    "    \n",
    "    def step(self, letter):\n",
    "        \"\"\"Take action (guess letter)\n",
    "        Returns: (next_state, reward, done)\n",
    "        \"\"\"\n",
    "        if letter in self.guessed or self.done:\n",
    "            return self.get_state(), -10, self.done  # Invalid action penalty\n",
    "        \n",
    "        self.guessed.add(letter)\n",
    "        \n",
    "        if letter in self.word:\n",
    "            # Correct guess\n",
    "            self.masked = ''.join([c if c in self.guessed else '_' for c in self.word])\n",
    "            reward = 2  # reward for correct guess\n",
    "            \n",
    "            if '_' not in self.masked:\n",
    "                # Won the game\n",
    "                self.done = True\n",
    "                reward = 50  # Big reward for winning\n",
    "        else:\n",
    "            # Wrong guess\n",
    "            self.lives -= 1\n",
    "            reward = -1  # Small penalty for wrong guess\n",
    "            \n",
    "            if self.lives == 0:\n",
    "                # Lost the game\n",
    "                self.done = True\n",
    "                reward = -30  # Penalty for losing\n",
    "        \n",
    "        return self.get_state(), reward, self.done\n",
    "\n",
    "print(\"✓ HangmanEnv defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d87a5",
   "metadata": {},
   "source": [
    "## 4. DQN Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3659b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network for Hangman\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim=26, hidden_dim=256):\n",
    "        super(DQN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for DQN\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=50000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.FloatTensor(states).to(device),\n",
    "            torch.LongTensor(actions).to(device),\n",
    "            torch.FloatTensor(rewards).to(device),\n",
    "            torch.FloatTensor(next_states).to(device),\n",
    "            torch.FloatTensor(dones).to(device)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "print(\"✓ DQN and ReplayBuffer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a302dd3c",
   "metadata": {},
   "source": [
    "## 5. State Encoding & DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37787d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_state(state, max_word_len=20):\n",
    "    \"\"\"Encode state as fixed-size feature vector\n",
    "    Features: masked word one-hot + guessed bitmap + lives\n",
    "    \"\"\"\n",
    "    masked = state['masked']\n",
    "    guessed = state['guessed']\n",
    "    lives = state['lives']\n",
    "    \n",
    "    # One-hot encode masked word (pad/truncate to max_word_len)\n",
    "    # Use 27 features per position: 26 letters + 1 for '_'\n",
    "    word_encoding = []\n",
    "    for i in range(max_word_len):\n",
    "        if i < len(masked):\n",
    "            char = masked[i]\n",
    "            if char == '_':\n",
    "                one_hot = [0] * 26 + [1]\n",
    "            else:\n",
    "                idx = ord(char) - ord('a')\n",
    "                one_hot = [0] * 27\n",
    "                one_hot[idx] = 1\n",
    "            word_encoding.extend(one_hot)\n",
    "        else:\n",
    "            word_encoding.extend([0] * 27)  # padding\n",
    "    \n",
    "    # Guessed letters bitmap (26 bits)\n",
    "    guessed_encoding = [1 if chr(ord('a') + i) in guessed else 0 for i in range(26)]\n",
    "    \n",
    "    # Lives remaining (normalized)\n",
    "    lives_encoding = [lives / 6.0]\n",
    "    \n",
    "    # Combine all features\n",
    "    state_vector = word_encoding + guessed_encoding + lives_encoding\n",
    "    return np.array(state_vector, dtype=np.float32)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent for Hangman\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim=26, lr=0.001, gamma=0.95, \n",
    "                 epsilon_start=1.0, epsilon_end=0.05, epsilon_decay=0.995):\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # Q-networks\n",
    "        self.policy_net = DQN(state_dim, action_dim).to(device)\n",
    "        self.target_net = DQN(state_dim, action_dim).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.memory = ReplayBuffer(capacity=50000)\n",
    "        \n",
    "    def select_action(self, state_dict, available_actions=None):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        state = encode_state(state_dict)\n",
    "        \n",
    "        if available_actions is None:\n",
    "            available_actions = list(state_dict['available'])\n",
    "        \n",
    "        if not available_actions:\n",
    "            return None\n",
    "        \n",
    "        # Epsilon-greedy\n",
    "        if random.random() < self.epsilon:\n",
    "            # Random action from available\n",
    "            letter = random.choice(available_actions)\n",
    "            return letter\n",
    "        else:\n",
    "            # Greedy action\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                q_values = self.policy_net(state_tensor)[0]\n",
    "                \n",
    "                # Mask out unavailable actions\n",
    "                for i in range(26):\n",
    "                    letter = chr(ord('a') + i)\n",
    "                    if letter not in available_actions:\n",
    "                        q_values[i] = float('-inf')\n",
    "                \n",
    "                action_idx = q_values.argmax().item()\n",
    "                return chr(ord('a') + action_idx)\n",
    "    \n",
    "    def train_step(self, batch_size=128):\n",
    "        \"\"\"Train on a batch from replay buffer\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(batch_size)\n",
    "        \n",
    "        # Current Q values\n",
    "        q_values = self.policy_net(states)\n",
    "        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states)\n",
    "            next_q_value = next_q_values.max(1)[0]\n",
    "            target_q_value = rewards + (1 - dones) * self.gamma * next_q_value\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.MSELoss()(q_value, target_q_value)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from policy to target network\"\"\"\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay epsilon\"\"\"\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "print(\"✓ DQNAgent defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb63d0ab",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DQN agent\n",
    "STATE_DIM = 20 * 27 + 26 + 1  # (max_word_len * 27) + guessed_bitmap + lives\n",
    "agent = DQNAgent(state_dim=STATE_DIM, lr=0.0005, gamma=0.95)\n",
    "\n",
    "# Training parameters\n",
    "NUM_EPISODES = 2000\n",
    "BATCH_SIZE = 128\n",
    "TARGET_UPDATE_FREQ = 50\n",
    "TRAIN_FREQ = 4\n",
    "\n",
    "# Training metrics\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "losses = []\n",
    "win_rates = []\n",
    "\n",
    "print(\"Starting DQN training...\")\n",
    "print(f\"Episodes: {NUM_EPISODES}, Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "for episode in tqdm(range(NUM_EPISODES), desc=\"Training\"):\n",
    "    # Sample random word from corpus\n",
    "    word = random.choice(corpus)\n",
    "    env = HangmanEnv(word)\n",
    "    \n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_len = 0\n",
    "    step = 0\n",
    "    \n",
    "    while not env.done:\n",
    "        # Select and perform action\n",
    "        action_letter = agent.select_action(state)\n",
    "        if action_letter is None:\n",
    "            break\n",
    "        \n",
    "        next_state, reward, done = env.step(action_letter)\n",
    "        \n",
    "        # Store transition\n",
    "        state_encoded = encode_state(state)\n",
    "        next_state_encoded = encode_state(next_state)\n",
    "        action_idx = ord(action_letter) - ord('a')\n",
    "        \n",
    "        agent.memory.push(\n",
    "            state_encoded,\n",
    "            action_idx,\n",
    "            reward,\n",
    "            next_state_encoded,\n",
    "            float(done)\n",
    "        )\n",
    "        \n",
    "        # Train if enough samples\n",
    "        if step % TRAIN_FREQ == 0:\n",
    "            loss = agent.train_step(BATCH_SIZE)\n",
    "            if loss > 0:\n",
    "                losses.append(loss)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        episode_len += 1\n",
    "        step += 1\n",
    "    \n",
    "    # Update target network\n",
    "    if episode % TARGET_UPDATE_FREQ == 0:\n",
    "        agent.update_target_network()\n",
    "    \n",
    "    # Decay epsilon\n",
    "    agent.decay_epsilon()\n",
    "    \n",
    "    # Record metrics\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(episode_len)\n",
    "    \n",
    "    # Calculate win rate over last 100 episodes\n",
    "    if episode >= 99:\n",
    "        recent_wins = sum(1 for i in range(episode-99, episode+1) if episode_rewards[i] > 0)\n",
    "        win_rates.append(recent_wins / 100.0)\n",
    "    \n",
    "    # Print progress\n",
    "    if (episode + 1) % 200 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-200:])\n",
    "        avg_length = np.mean(episode_lengths[-200:])\n",
    "        win_rate = win_rates[-1] if win_rates else 0\n",
    "        print(f\"\\nEpisode {episode+1}/{NUM_EPISODES}\")\n",
    "        print(f\"  Avg Reward: {avg_reward:.2f}\")\n",
    "        print(f\"  Avg Length: {avg_length:.2f}\")\n",
    "        print(f\"  Win Rate: {win_rate:.2%}\")\n",
    "        print(f\"  Epsilon: {agent.epsilon:.3f}\")\n",
    "        print(f\"  Replay Buffer: {len(agent.memory)}\")\n",
    "\n",
    "print(\"\\n✓ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c010f25",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b94d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Episode rewards\n",
    "axes[0, 0].plot(episode_rewards, alpha=0.3, label='Raw')\n",
    "if len(episode_rewards) > 50:\n",
    "    smoothed = pd.Series(episode_rewards).rolling(50).mean()\n",
    "    axes[0, 0].plot(smoothed, label='Smoothed (50-ep avg)', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].set_title('Episode Rewards')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Episode lengths\n",
    "axes[0, 1].plot(episode_lengths, alpha=0.3, label='Raw')\n",
    "if len(episode_lengths) > 50:\n",
    "    smoothed = pd.Series(episode_lengths).rolling(50).mean()\n",
    "    axes[0, 1].plot(smoothed, label='Smoothed (50-ep avg)', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Episode Length')\n",
    "axes[0, 1].set_title('Episode Lengths (Guesses per Game)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Training loss\n",
    "if losses:\n",
    "    axes[1, 0].plot(losses, alpha=0.3, label='Raw')\n",
    "    if len(losses) > 100:\n",
    "        smoothed = pd.Series(losses).rolling(100).mean()\n",
    "        axes[1, 0].plot(smoothed, label='Smoothed (100-step avg)', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Training Step')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].set_title('Training Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 4: Win rate\n",
    "if win_rates:\n",
    "    axes[1, 1].plot(range(99, len(episode_rewards)), win_rates, linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Episode')\n",
    "    axes[1, 1].set_ylabel('Win Rate')\n",
    "    axes[1, 1].set_title('Win Rate (100-episode window)')\n",
    "    axes[1, 1].set_ylim([0, 1])\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dqn_training_progress.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training progress visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df410081",
   "metadata": {},
   "source": [
    "## 8. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57486ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set agent to evaluation mode (epsilon = 0 for pure exploitation)\n",
    "agent.epsilon = 0.0\n",
    "\n",
    "print(\"Evaluating DQN agent on test set...\")\n",
    "wins = 0\n",
    "total_wrong_guesses = 0\n",
    "\n",
    "for word in tqdm(test_words, desc=\"Evaluating\"):\n",
    "    env = HangmanEnv(word)\n",
    "    state = env.reset()\n",
    "    wrong_guesses = 0\n",
    "    \n",
    "    while not env.done:\n",
    "        action_letter = agent.select_action(state)\n",
    "        if action_letter is None:\n",
    "            break\n",
    "        \n",
    "        next_state, reward, done = env.step(action_letter)\n",
    "        \n",
    "        if reward < 0:  # Wrong guess\n",
    "            wrong_guesses += 1\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    if '_' not in env.masked:  # Won\n",
    "        wins += 1\n",
    "    \n",
    "    total_wrong_guesses += wrong_guesses\n",
    "\n",
    "# Calculate metrics\n",
    "success_rate = wins / len(test_words)\n",
    "avg_wrong = total_wrong_guesses / len(test_words)\n",
    "final_score = wins * 100 - total_wrong_guesses * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DQN AGENT EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Games: {len(test_words)}\")\n",
    "print(f\"Wins: {wins} ({success_rate:.2%})\")\n",
    "print(f\"Total Wrong Guesses: {total_wrong_guesses}\")\n",
    "print(f\"Avg Wrong Guesses: {avg_wrong:.3f}\")\n",
    "print(f\"\\nFINAL SCORE: {final_score:.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a13c0",
   "metadata": {},
   "source": [
    "## 9. Comparison with Other Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba989145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison data (from previous notebooks)\n",
    "comparison_data = {\n",
    "    'Agent': ['Original HMM', 'RL + HMM', 'Improved HMM', 'Enhanced N-gram', 'DQN'],\n",
    "    'Success Rate': [19.80, 19.90, 24.60, 35.70, success_rate * 100],\n",
    "    'Final Score': [-55324, -55302, -53878, -50471, final_score]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "df_comparison = df_comparison.sort_values('Success Rate', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AGENT PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Success rates\n",
    "colors = ['#9467bd', '#ff7f0e', '#2ca02c', '#d62728', '#1f77b4']\n",
    "bars1 = ax1.bar(df_comparison['Agent'], df_comparison['Success Rate'], \n",
    "                color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Success Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Success Rate Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax1.set_ylim(0, max(df_comparison['Success Rate']) * 1.2)\n",
    "ax1.tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Add value labels\n",
    "for bar, rate in zip(bars1, df_comparison['Success Rate']):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{rate:.2f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Scores\n",
    "bars2 = ax2.bar(df_comparison['Agent'], df_comparison['Final Score'], \n",
    "                color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.set_ylabel('Final Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Final Score Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax2.tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Add value labels\n",
    "for bar, score in zip(bars2, df_comparison['Final Score']):\n",
    "    height = bar.get_height()\n",
    "    y_pos = height - 1500 if height < 0 else height + 500\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., y_pos,\n",
    "             f'{score:.0f}', ha='center', va='top' if height < 0 else 'bottom', \n",
    "             fontweight='bold', fontsize=9, color='white' if height < 0 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dqn_agent_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Comparison visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0126de0",
   "metadata": {},
   "source": [
    "## 10. Save DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35be328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained DQN model\n",
    "torch.save({\n",
    "    'policy_net_state_dict': agent.policy_net.state_dict(),\n",
    "    'target_net_state_dict': agent.target_net.state_dict(),\n",
    "    'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "    'epsilon': agent.epsilon,\n",
    "    'state_dim': STATE_DIM,\n",
    "    'training_stats': {\n",
    "        'episodes': NUM_EPISODES,\n",
    "        'final_success_rate': success_rate,\n",
    "        'final_score': final_score,\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'win_rates': win_rates\n",
    "    }\n",
    "}, 'dqn_agent.pth')\n",
    "\n",
    "print(\"✓ DQN model saved to 'dqn_agent.pth'\")\n",
    "\n",
    "# Save results summary\n",
    "results_summary = f\"\"\"\n",
    "DQN Agent Results Summary\n",
    "=========================\n",
    "\n",
    "Training Configuration:\n",
    "- Episodes: {NUM_EPISODES}\n",
    "- Batch Size: {BATCH_SIZE}\n",
    "- Learning Rate: 0.0005\n",
    "- Gamma: 0.95\n",
    "- Epsilon Decay: 0.995\n",
    "- State Dimension: {STATE_DIM}\n",
    "- Device: {device}\n",
    "\n",
    "Evaluation Results:\n",
    "- Test Set Size: {len(test_words)} words\n",
    "- Success Rate: {success_rate:.2%}\n",
    "- Wins: {wins}/{len(test_words)}\n",
    "- Total Wrong Guesses: {total_wrong_guesses}\n",
    "- Avg Wrong Guesses: {avg_wrong:.3f}\n",
    "- Final Score: {final_score:.2f}\n",
    "\n",
    "Comparison with Other Agents:\n",
    "{df_comparison.to_string(index=False)}\n",
    "\n",
    "Model saved to: dqn_agent.pth\n",
    "\"\"\"\n",
    "\n",
    "with open('dqn_results.txt', 'w') as f:\n",
    "    f.write(results_summary)\n",
    "\n",
    "print(\"✓ Results summary saved to 'dqn_results.txt'\")\n",
    "print(\"\\n\" + results_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
